你好，我是郑晔！

上一讲，我们站在用户视角介绍了LLM，这个视角可以帮助我们更好地理解如何使用大模型。

不过，站在用户视角，我们只能关心到语言输入和输出，而如果要开发一个 AI 应用，我们不可避免地会接触到其它一些概念，比如，Token、Embedding、温度等等，这些概念是什么意思呢？这一讲，我们就从技术的视角看一下大模型，到这一讲的末尾，你也就知道这些概念是怎么回事了。

在出发之前，我要强调一下，我们不是为了打造一个大模型，而是为了更好地理解应用开发中的各种概念。好，我们开始！

## 技术视角的大模型

站在技术视角理解大模型，核心就是搞懂一件事，大模型到底做了些什么。其实，大模型的工作很简单， **一次添加一个词**。

怎么理解这个说法呢？本质上说，ChatGPT做的是针对任何文本产生“合理的延续”。所谓“合理”，就是“人们看到诸如数十亿个网页上的内容后，可能期待别人会这样写”。我们借鉴 Stephen Wolfram 的《这就是 ChatGPT》（What Is ChatGPT Doing … and Why Does It Work?）里的一个例子一起来看一下。

### 选择下一个词

假设我们手里的文本是“The best thing about AI is its ability to”（AI 最棒的地方在于它能）。想象一下，我们浏览了人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后，猜测一下接下来要出现的是什么词，以及这些词出现的概率是多少。

实际上，GPT 做的就是类似的事情，只不过它查看的不是字面意义上的文本，而是寻找在某种程度上“意义匹配”的事物。最终的结果是，它会列出随后可能出现的词及其出现的“概率”（按“概率”从高到低排列）：

![](https://static001.geekbang.org/resource/image/a5/c8/a5a73f77bc9760606cac114c89a48cc8.jpg?wh=3000x902)

有了这个带有概率的词列表，我们就会从这些词中选择一个词。接下来，我们再把这个词附加到我们的文本上，再次询问大模型下一个词是什么。本质上，我们生成的一段内容就是这样一遍一遍地询问“下一个词是什么”，不断地重复这个过程。

下面就是一个例子，从这个例子中，我们可以清晰地看到文本不断生成的过程。

![](https://static001.geekbang.org/resource/image/0e/b6/0e0197a0fa059a0042b119335aa8ecb6.jpg?wh=5113x2234)

这里有一个问题需要澄清一下，虽然我们这里说的是添加一个词，但严格意义上说，每次添加的是一个 Token。 **Token** 就是我们理解大模型编程的第一个重要概念。这个 Token 可能是我们传统理解的一个完整的单词，可能是一个单词组合，甚至可能是单词的一部分（这就是大模型可以“造出新词”的原因）。

Token 的概念在大模型编程中是非常重要的，现在各大厂商的竞争中，有一个很重要的指标就是上下文窗口（Context Window）的大小。这里的上下文窗口，指的就是大模型可以处理 Token 数量，上下文越大，能处理的 Token 越多。能处理的 Token 越多，大模型对信息的理解就越充分，生成的内容就越接近我们需要的结果。所以，现在各个大模型比拼的其中一项就是上下文窗口的大小。

下面是 gpt-4o-mini 的上下文窗口处理的 Token 情况（摘自 Open AI 官网）。

![](https://static001.geekbang.org/resource/image/c1/8c/c1de531e4c1ede7ac0e0d3fa01af198c.jpg?wh=3088x1112)

除此之外，Token 还有一个非常现实的作用，就是计费，现在的大模型编程都是根据 Token 进行收费，Token 越多收费越高。下面是 gpt-4o-mini 的的计费情况（摘自 Open AI 官网），可以看出它就是根据 Token 计算的。

![](https://static001.geekbang.org/resource/image/12/0d/1223b526b37b1908a7fc1f68cb45fa0d.jpg?wh=1612x510)

### 引入随机性

好，我们回到内容生成的过程中。前面我们说了，我们会从一个带有概率的词列表中进行选择。接下来，我们需要确定选择哪个词添加到我们的内容中。

有人认为应该选择“概率最高”的词。但是，如果我们总是选择排名最高的词，通常会得到一篇非常“平淡”的文章，完全显示不出任何“创造力”。这就好比你问别人一个问题，他每次都给你同样的回答，你一定会觉得这个人非常的无趣。所以，在选择的时候，我们可以有一些随机性，选择排名较低的词，这样我们就可以得到一篇“更有趣”的文章。

有了随机性，也意味对于同样的提示词，我们每次得到的内容可能会不同。为了达到这样的效果，我们就引入了一个表示随机性强弱的概念： **温度（Temperature）**。这是大模型编程中另一个重要的概念。

很显然，温度这个概念借鉴自物理学，不过，它和物理学之间并没有实际的联系。站在理解这个概念的角度，你可以认为它是表示大模型活跃程度的一个参数，通过调节这个参数，大模型变得更加活跃，或是更加死板。

在大模型编程中，温度是一个非常重要的概念，温度设定的情况很大程度上决定了大模型给出怎样的表现。在 OpenAI 的 API 中，这个参数越小，表示确定性越强，越大，表示随机性越强，简单理解就是，温度越高越活跃。

下面就是有了“温度”之后，对于同样的提示词，就产生了不同的结果。

![](https://static001.geekbang.org/resource/image/24/6a/244970e9ae3e9df1daceab07cbf26e6a.jpg?wh=1279x485)

到这里，你已经对大模型生成文本的过程有了一个最基本的了解。不过，这里还有一个细节需要解释一下，这里面会牵扯到一个在大模型编程中的另一个重要概念：Embedding。

### 从字符串到向量

虽然我们给大模型提供的是一个待补齐的字符串，但实际上，在大模型内部处理的并不是字符串，而是向量。之所以要将字符串转换为向量，简单理解，就是现在大部分的 AI 算法只支持向量。

你可能会好奇，为啥不支持字符串呢？因为字符串只是这些 AI 算法的一种输入，我们还可以输入图片、输入视频，甚至各种各样的信息，只要把这些输入都转换成向量，AI 算法都可以轻松地驾驭。由此，你可以知道，交给 AI 算法处理的前提就是 **把各种信息转换成向量**。

理解了向量的意义，接下来的问题就是，字符串是怎样变成向量的？

在大模型的处理过程中，字符串转成向量会经历两个步骤，第一步叫 One-Hot 编码，第二步则是把编码的结果进行压缩。

One-Hot 编码就是将离散的分类值转换为二进制向量。比如，我们有一个颜色的分类值，分别是红（red）、绿（green）、蓝（blue），其编码如下所示：

![](https://static001.geekbang.org/resource/image/c2/67/c24cf7c5736d9203c980aff263566e67.jpg?wh=5787x2080)

在这个表格中，我们可以看到红绿蓝在向量里各占一位，有值则为 1，没有值对应 0，所以，红对应的向量是\[1, 0, 0\]、绿对应的是\[0, 1, 0\]，而蓝对应的是\[0, 0, 1\]。当我们输入是“红绿红蓝”，就会得到对应的四个向量：\[1, 0, 0\]、\[0, 1, 0\]、\[1, 0, 0\]、\[0, 0, 1\]。这就是最简单的One-Hot 编码.

在大模型中，离散的值是什么呢？就是我们前面提到的 Token。我们把字符串转成 Token 列表，每个 Token 都会自己对应的一个唯一标识，而这个标识就是向量中的内容。你或许已经想到了，在一个有规模的语料库中，Token 数量会非常多，下面是一个示例：

![](https://static001.geekbang.org/resource/image/68/a2/6822bd5ec8e7b176fd83cddaa775d5a2.jpg?wh=1954x674)

从这个图中，我们就可以看到，这是典型的稀疏存储，也就是说，一个向量中就有大量的 0 存在，真正有含义的位并不多。我们文本中的每个 Token 都会对应一个向量，可想而知，这里面的 0 会有多少。所以，我们会经历下一个处理过程，对这个编码结果进行压缩，得到我们想要的 Embedding。

获取 Embedding 常用的方法有两种，降维和把 Embedding 训练成神经网络。降维的技术有很多，比如主成分分析（Principal component analysis）。但在大模型中，通常使用把 Embedding 训练成神经网络的方法，也就是把我们前一步得到的向量送给一个神经网络，得到最终的压缩过的向量。具体如何训练是一个更为复杂的过程，这不是我们这里关注的重点。

你只需要知道，经过这个处理之后，一个硕大的向量就会压缩成一个固定大小的向量。比如，在 GPT 3 中，每个 Token 都由 768 个数字组成的向量表示。之后，再把压缩过的向量作为输入传给大模型，用来生成最终的结果，这时才轮到我们听说过的神经网络、自注意力机制这些东西登场。不过，对于我们要编写大模型应用来说，这些东西就超过了基本的范畴，你稍微有个印象就够了。

如果你对 GPT 的详细运作原理感兴趣，我给你推荐几篇文章看看。

> [Inside GPT — I : Understanding the text generation](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093)
>
> [Inside GPT — II : The core mechanics of prompt engineering](https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da)

## 总结时刻

这一讲，我们站在开发大模型应用的视角来理解大模型是如何运作的。我们知道了大模型的核心任务是每次添加一个词，经过了上面的学习，你应该清楚，更准确的说法是每次添加一个 Token。Token 在大模型编程中非常重要，上下文窗口的大小决定了大模型一次可以处理多少个 Token，大模型通常是用 Token 进行计费的。

在生成内容的过程中，为了让内容拥有更多的创造性，需要引入随机性。由此我们引入了一个温度的概念，表示大模型的活跃程度。

大模型内部处理的并不是字符串，而是向量。文本向量化之后，才能真正进入大模型的处理，这个过程就是一个 Embedding 的过程。在大模型的处理中，需要经过 One-Hot 编码，然后，再对 One-Hot 编码之后的结果进行压缩，得到我们最终需要的结果。

如果今天的内容你只能记住一件事，那请记住： **大模型的工作就是一次添加一个 Token**。

## 思考题

我们今天讨论了大模型内部的一些工作状况，你看过哪些介绍大模型工作原理的内容呢？欢迎在留言区分享你的学习经验。

## 参考资料

Inside GPT — I : Understanding the text generation

[https://towardsdatascience.com/inside-gpt-i-1e8840ca8093](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093)

Inside GPT — II : The core mechanics of prompt engineering

[https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da](https://towardsdatascience.com/inside-gpt-ii-why-exactly-does-your-prompt-matter-1aea1aef35da)

嵌入

[https://developers.google.com/machine-learning/crash-course/embeddings?hl=zh-cn](https://developers.google.com/machine-learning/crash-course/embeddings?hl=zh-cn)