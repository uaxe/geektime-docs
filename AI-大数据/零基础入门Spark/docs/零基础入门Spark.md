说到学习Spark，如果你对“Spark还有那么火吗？会不会已经过时了？”这个问题感到困惑，那大可不必。

因为经过十多年的发展，Spark已经由当初的“大数据新秀”成长为数据应用领域的中流砥柱，早已成为各大头部互联网公司的标配。比如，字节跳动、美团、Netflix等公司基于Spark构建的应用，在为公司旗下的核心产品提供服务。

这也就意味着， **对于数据应用领域的任何一名工程师来说，Spark开发都是一项必备技能**。

虽然Spark好用，而且是大数据从业者的一门必修课，但对于入门这件事儿，却也面临着这样一些难题：

- 学习资料多且杂，自己根本就梳理不出脉络，更甭提要构建结构化的知识体系了。
- 学习Spark，一定要先学Scala吗？新学一门编程语言，真不是件容易的事儿。
- Spark的开发算子太多，记不住，来了新的业务需求，又不知道该从哪里下手。
- ……

那么，该如何解决这些问题，从而打开Spark应用开发的大门呢？

为此，我们邀请到了吴磊老师。他会结合自己这些年学习、应用和实战Spark的丰富经验，为你梳理一套零基础入门Spark的“三步走”方法论： **熟悉Spark开发API与常用算子、吃透Spark核心原理、玩转Spark计算子框架**，从而帮助你零基础上手Spark 。

这个“三步走”方法论再配合4个不同场景的小项目，吴磊老师会从基本原理到项目落地，带你深入浅出玩转Spark。

### 课程模块设计

结合Spark最常用的计算子框架，这门课设计为4个模块，它与“三步走”方法论的对应关系如下：

![](https://static001.geekbang.org/resource/image/d5/4f/d54a508d57ce3ecab7d1f262b9dfb34f.jpg)

**基础知识模块**：从一个叫作“Word Count”的小项目开始，详细地讲解RDD常用算子的含义、用法与适用场景，以及RDD编程模型、调度系统、Shuffle管理、内存管理等核心原理，帮你打下坚实的理论基础。

**Spark SQL模块**：从“小汽车摇号”项目入手，带你熟悉Spark SQL开发API，为你讲解Spark SQL的核心原理与优化过程，以及Spark SQL与数据分析有关的部分，如数据的转换、清洗、关联、分组、聚合、排序，等等。

**Spark MLlib模块**：从“房价预测”这个小项目入手，带你了解Spark在机器学习中的应用，深入学习Spark MLlib丰富的特征处理函数和它支持的模型与算法，并带你了解Spark + XGBoost集成是如何帮助开发者应对大多数的回归与分类问题。

**Structured Streaming模块**：重点讲解Structured Streaming是怎么同时保证语义一致性与数据一致性的，以及如何应对流处理中的数据关联，并通过Kafka + Spark这对“Couple”的系统集成，来演示流处理中的典型计算场景。