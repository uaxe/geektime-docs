你好，我是 Tyler。

说起 ChatGPT，你对它的印象是什么呢？

据我观察，不同人有不同的看法。有些人不太高兴，因为担心它的代码能力会对工程师的工作造成威胁。另一些人的态度是不屑一顾，认为它只会说些不切实际的东西。还有一些人正在学习一些易上手、但欠缺深度的教程，用它建立知识库，自动化日常工作，觉得“小小GPT不过如此”，很容易就能“拿捏”它了。

不过，这一轮技术革命并不是冲着这“仨瓜俩枣”来的，未来所有数字化工作都可能被大模型技术颠覆。所以今天我会带你正本清源，看看这一轮技术革命的真正“终局”是什么样子。

好大的口气，你可能会这么想。的确，在一个技术快速发展的阶段下断言，确实有些“冒险”，但我非常有信心与你分享我的判断，因为这已几乎成为了全球大模型工作者的共识。

![](https://static001.geekbang.org/resource/image/dd/7b/ddd8dd2581f008d04372e590c15fd07b.jpg?wh=3800x2138)

如上图所示，我们未来所面临的终局形态，不只是一个能说会道的聊天机器人，而是“具身智能”的通用人工智能。所谓具身智能，就是像人一样能与环境交互感知，自主规划、决策、行动的机器人。

## ChatGPT 的发展简史

像人一样？你可能有点半信半疑，但别以为这是科幻小说虚构的故事。如果你足够细心，完全可以在 OpenAI 的发展过程中发现一些端倪。我现在就带你回顾一下GPT发展的过程当中有哪些重要历史节点，看看它是怎样一步步进化的。

GPT的诞生要归功于NLP的快速发展。从2018年到2021年，是第一代大语言模型（LLM）的“技术爆炸”期。人们逐渐学会了，如何使用海量的无标签数据，来训练这些“涌现”智能的大模型。随后，OpenAI采用强化学习技术，点亮了LLM的智能，ChatGPT由此横空出世。

如果你对前面的故事已经非常熟悉了，那说明你已超过95%侃侃而谈大模型技术的人了。

但实际上，真正的故事才刚刚开始。我画了一张OpenAI产品发展的示意图，你可以猜测一下他们之间的关系，接下来，我将会为你道出其中的逻辑。

![](https://static001.geekbang.org/resource/image/d0/f3/d0f1fc84f15d0c572bdcb84c0ec11bf3.jpg?wh=3800x2138)

## 猩球崛起（使用工具）

人类之所以在地球上显得独特，一个重要原因是我们更擅长使用工具。对于已经掌握语言能力的大型语言模型（LLM）来说，学会使用工具只是一个时间问题，而且它们的学习速度比我们想象的还要快。

从OpenAI产品的各种迹象来看， **他们正在教导LLM使用甚至创造工具**。

首先，OpenAI推出了插件和联网功能，弥补了大型语言模型自身记忆的不足。这标志着LLM正式开始学习使用工具。

随后推出的函数功能，意味着LLM已经学会使用API来完成复杂任务，在此之前这可是后端工程师的主要工作。最后推出的代码编辑器，让LLM直接学会制造工具， **这几乎动摇了所有工程师的饭碗**。

虽然人类可以掌控使用工具的大模型，但令人既兴奋又担心的是，研究人员还找到了 **让LLM “思考”的方法**。我们这就来看看这人造的思维能力是怎样的！

## 西部世界（自我思考）

在美剧《西部世界》中，机器人乐园的创始人偷偷地为机器人植入了“意识”代码，导致机器人自我对话，从而产生了自我意识。在真实世界中，类似的事情正在发生。各大机构正在制造能够自我对话的“智能体”，从而彻底解放人类的脑力工作。

你可以回想一下，当人类面临一个任务的时候，会怎么做？

首先，我们会思考任务的主要步骤有哪些，然后调取相关的资料，形成可行的方案，接着通过分工去执行具体的事项，最后汇总完成任务。对于智能体来说也是一样，只不过要将之前人类大脑的工作交给大语言模型（LLM）完成。

## 技术要点

在这里，我为你准备了一张智能体的结构图。为了让智能体具备“自治”的能力，一般需要将计划、记忆和工具这三部分组合起来。

![](https://static001.geekbang.org/resource/image/ce/d8/ce92102a8b7a9a0dcb1yy59cdf1636d8.jpg?wh=3800x1884)

接下来，我们就沿着这三个部分看看有哪些重点工作，当你把它们背后的逻辑弄清楚之后，后面课程里学习具体细节时，才能做到心中有数。

### 任务规划（Planning）

教大型语言模型（LLM）思考的过程，有点像苏格拉底的“产婆术”。苏格拉底认为，他不能代替别人思考，但他可以通过提问引导别人思考，就好像产婆引导孕妇一样。

这种方法可以让LLM对自己的想法进行调整和反思，最经典的方法是ReAct，他有三个概念：

- **Thought：** 表示让大语言模型思考，目前需要做哪些行为，行为的对象是谁，它要采取的行为是不是合理的。
- **Act：** 也就是针对目标对象，执行具体的动作，比如调用API这样的动作，然后收集环境反馈的信息。
- **Obs：** 它代表把外界观察的反馈信息，同步给大语言模型，协助它做出进一步的分析或者决策。

你可以看看文稿后面的这张截图，图里展示的是ReAct的一个示例，它可以帮你加深理解。

![](https://static001.geekbang.org/resource/image/78/65/7888b2f8e9213e3eb01c47a3ee27f765.jpg?wh=2313x1876)

我们可以用这种方法来启发LLM工作，比如让它帮你制定工作方案，并持续向它提问，例如：你的执行步骤有哪些潜在隐患和风险、有哪些方法可以降低风险、能否帮助我制定一些安全风险预案等等问题，以确保它生成的内容安全可靠。

在这个过程中，你要尽量唤醒LLM的相关知识，生成合理的计划，此时 **思维链技术（CoT）** 就非常重要了，它可以让LLM将任务分解为可解释的步骤。更多关于这些技术的细节，我们将在第三、第四章展开讨论。在此阶段，你只需掌握LLM在任务规划里如何使用即可。

### 记忆唤醒（Memory）

无论是在制定计划、使用工具或执行任务的过程中，LLM 都需要外部信息的帮助来辅助进行思考。为了更好地让 LLM 拥有记忆力，我们不妨先参考一下人类是如何记忆的。

在神经科学研究中，人类的记忆可分为感觉记忆、短期记忆和长期记忆三种类型。

- 感觉记忆，是人体接收到外部信号以后，瞬间保留的视觉、听觉、触觉的记忆片段，在AI系统中类似于高维嵌入表示，也就是我们常说的 “Embedding”。
- 短期记忆，是你 **当前意识中的信息**，在LLM中类似于提示词（Prompt）中的所有信息。
- 长期记忆，包含了 **你能回忆的所有信息**，在LLM中类似于外部向量存储。

LLM 能“消化”的，只有提示词（Prompt）中的短时记忆，所以你只需要在长期记忆中选择最重要的内容放入提示词，这里我给出一张图方便你理解这个过程。

- 首先，LLM 在得到任务后，会帮助你制定记忆唤醒方案。
- 然后，AI系统执行该方案，生成相关的查询指令，从外部数据中查询数据。
- 最后，我们将这些数据交给 LLM 来判断是否已获得足够完成任务的数据。如果没有，LLM 会生成新的唤醒方案，并循环这个过程。

![](https://static001.geekbang.org/resource/image/51/a1/51a11e38460ba1340fcbb00cdc2af0a1.jpg?wh=3900x1962)

现在，LLM 不但能制定任务规划，还能调取外部知识了。仅这两个能力足以让它自动化地完成很多脑力工作了。如果再让它学会使用工具，那潜力简直不可限量。我们再来聊一聊 LLM 使用工具的方法。

### 驾驭工具（Tools）

**要让 LLM 学会使用工具，首先需要让它认识工具**，比如TALM、Toolformer和Gorilla等方法让LLM 学会理解API的调用注释，这是Plugin和Function等功能的基础。

下图展示了Gorilla教会LLM使用API的全过程。

- 首先，我们使用大量API调用代码和文档作为语料，训练一个可以理解API的大语言模型。
- 然后，AI系统还将对这些API进行向量化操作，将它们存储在向量数据库中作为外部记忆。
- 随后，当用户发起请求时候，AI系统会从外部记忆中，获取跟请求相关的 API 交给 LLM。
- 最后，LLM 组合串联这些 API 形成代码，并执行代码，完成API的调用，生成执行结果。

![](https://static001.geekbang.org/resource/image/78/d1/789802f9d4090a75f92e690b60a05dd1.jpg?wh=3900x2127)

此外，根据Google的 [线报](https://medium.com/@daniellefranca96/gpt4-all-details-leaked-48fa20f9a4a)，GPT-4使用了MoE（混合专家模型）技术，由十多个领域专家模型共同提供服务。所以，它需要上层LLM有能力为指定的需求，找到最合适的专家模块，这其实也是一种LLM使用工具的能力。同样，就像前面提到的，OpenAI 发布的 Code Interpreter，也正式宣告 LLM 制作工具的时代已经来临。

## 小结

相信通过今天的学习，你已经知道了这一轮技术革命的目的是什么，而且不仅OpenAI在进行这些工作，国内的各大科技公司也在积极探索。这节课的知识重点，你可以参考后面的导图来回顾。

我们通过“产婆术”让大型语言模型学会“思考”。我们让它学会制定和反思自己的计划，并教会它获取外部知识和使用工具的方法，让它可以独立完成复杂任务。

![](https://static001.geekbang.org/resource/image/3c/49/3cfc2b4c2a55dc1b21f3f5833c401e49.jpg?wh=3800x2138)

而且，LLM除了使用工具之外，已经可以制造工具了。OpenAI推出的Code Interpreter，让 LLM 有能力解决处理几乎各种模态的二进制数据，而不仅限于文字和图像。长远来看，这将颠覆现有软件工程的模式，Serverless 已经不新鲜了，Codeless才是未来的常态。

除此之外，还有很多重要的内容，比如大模型的分布式训练技术、多模态大模型的训练方法，以及实现可信AI，解决大模型幻觉的办法等等。其实这些相关技术，早在既有的AI系统中得到了很好的实践，这些内容也会在后续的课程中逐渐展开，敬请期待。

## 思考题

结合今天的内容，你觉得教会LLM制定计划，反思计划以及使用工具这几个方面，哪方面的训练难度更大？为什么？

恭喜你完成我们第2次打卡学习，期待你在留言区和我交流互动。也欢迎你把这节课分享给身边朋友，和 TA 一起学习进步。