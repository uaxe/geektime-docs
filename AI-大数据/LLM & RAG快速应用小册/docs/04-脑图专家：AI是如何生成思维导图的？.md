> 本门课程为精品小课，不标配音频

你好，我是王吕。今天我们讲一下极客时间是如何给文章增加思维导图功能的。

极客时间给文章添加思维导图功能的初衷，是想让用户打开文章的第一眼就可以看到整篇文章的结构，用最短的时间对文章有一个整体上的了解，这样在阅读的过程中，就可以随时知道自己当前阅读到了什么进度，以及所处的主题是什么。

![图片](https://static001.geekbang.org/resource/image/0d/40/0d34100b5738bc22f68ec14c882dfa40.jpeg?wh=1910x590)

整个思维导图是基于 markmap 格式，使用 LLM 按照层级提炼总结文章的要点，形成类似目录的结构，结构层级使用多个 # 表示，这点和 markdown 一样。前端拿到这个文本之后使用 markmap-js 插件直接可以解析成思维导图。

极客时间的稿件都是兼容 markdown 格式的文本，那么如何提取一篇长文的要点呢？

## 提炼文章要点

受限于 LLM 的 token 长度限制，第一步还是切割文本，切割文本的几种方法我在 [第02讲](https://time.geekbang.org/column/article/805476) 提到过，不过这里还有需要考虑的不同之处。

1. 在使用文章片段的时候，每次可能只引用一段或很少内容进行提炼总结，而且输出的文本长度很短，长度基本可控，所以切割片段可以较长。

2. 更加关注某一个段落的完整性，也就是在理想状态下，正好切割的每个片段都是一个独立的完整主题要点，使用LLM对这个片段就可以提炼出一个要点，多个片段组合在一起，就可以得到文章的思维结构。当然，这是理想情况。


现实中会有哪些问题呢？我们很难找到一种切割方法，恰好按照文章结构要点进行分割，而且文章往往前后段落还存在一定的联系或者逻辑关系。那么极客时间是如何做的呢？

我们使用了 Refine 的方式，也叫做滚雪球。因为极客时间的稿件都是基于 markdown 格式，所以我们可以把不同级别的标题当做锚点进行切割，这样能保证切割出来的都是一段一段的内容，切割完之后，我们从第一段开始，把第一段的内容投喂给 LLM，得到总结的内容之后，把总结内容和第二段结合在一起，再次投喂给 LLM，如此往复，直到遍历完所有段落。

![图片](https://static001.geekbang.org/resource/image/5e/7a/5ef5834469bd8dd9e4d2c1a844b4577a.jpeg?wh=1920x801)

这里我把极客时间用的 prompt（修改版）也提供给你，供你参考：

```plain
你是一位精通文本理解和信息组织的专家，任务是分析给定的一段文本，并从中提取新的要点，生成一份基于 Markmap 格式的思维导图。前文要点中可能已经包含部分 Markmap 格式的要点，你需要将新提取的要点与已有内容拼接在一起，并合并重复内容，保证输出的整体结构清晰。操作步骤如下：

1. 阅读整个文本，识别文本中的主要主题、子主题和关键细节。
2. 如果文本中已经有 Markmap 格式的内容，提取其中的要点，并整合新的内容。删除重复或多余的部分，避免信息冗余。
3. 按照逻辑层次组织信息，确保结构严谨且不超过 4 层。
4. 使用 Markmap 格式展示，确保各主题、子主题之间的逻辑关联清晰。

前文要点：
<|markmap|>

给定的文本为：
<|text|>

输出格式为：
# 主要主题
## 子主题1
### 子主题1的细节1
### 子主题1的细节2
## 子主题2
### 子主题2的细节1
#### 子主题2的子细节1

```

经过上述步骤，我们就得到了一篇文章的要点，通过 API 输出给前端，就得到了一个还不错思维导图。

## 优化细节

在整个处理过程中，我们还发现了几个小技巧，可以提升思维导图的效果，这里分享给你。

1. 关于思维导图层级：极客时间的每篇文章都会有编辑参与，精心校对审核，借此来保证文章的质量，所以文章本身的结构就非常清晰，所以我们的思维导图最终设置了 4 级深度，就基本可以覆盖所有的文章了。但是在你的业务场景中，这种方法可能需要微调一下。假如你的文章内容是以故事为主，文章是一条线性叙事逻辑，那么最终你提炼出来的层级就会很浅，同级节点之间的关系就会是线性关联，有先后顺序，在 prompt 里也要适当修改，提示 LLM 要注意这种关系，如果强行增加层级，会导致最小的节点偏离主题，造成困扰。

2. 关于 LLM 选型：在此时此刻，大模型已经变化非常大了，有很多大模型能承载的 token 长度已经达到了千万级，比如阿里的 `qwen-long`，这个长度已经远远超过一篇文章的长度了，如果选用了这类模型，可以直接把整篇文章投喂给它，然后把优化点都放到 prompt 上，这样实现起来就非常简单了。不过这里也要注意一个问题，这类模型虽然支持超长的 token处理，但是当 token 变长的时候，有些模型对内容的理解能力会下降。如果使用了这种方式，还是要多观察一些文章的生成结果，如果发现无论怎样优化 prompt，模型提炼的要点都不是很准确的话，那么这个模型可能就不太适合你的使用场景。

3. 关于结果验证：一般情况下，使用 LLM 生成思维导图结果之后，还是需要在后台由编辑或者运营人员审核调整之后再发布，这也是一些人工成本。针对这个问题，我们可以在生成思维导图之后，使用上述超长token模型，再对文章进行一遍提炼，然后让 LLM 去对比优化原来的思维导图结果，能尽量减少人工成本。

上述几点是我们在实际操作的过程中总结出来的一些方法和思考，希望能帮你节省一些时间，扩展一下思路。

## 拓展思考

现在我们基于思维导图这个案例，往上提升一下，探讨一下这个应用背后的方法还能应用在哪些地方。

纵观整个思维导图应用，不难看出，我们做的事情的本质，就是在压缩信息，即：把一篇文章的内容用一些关键词和短句表达，并且尽量保持原文的逻辑结构。这是大模型最擅长的能力，我们有了这把锤子，如何才能找到我们自己业务中的钉子呢？

我这里有几条建议，满足这些条件的话，可能你的这个场景就适合使用 LLM 压缩信息。

1. 识别信息过载：寻找那些信息量大，但需要快速理解和决策的业务领域。

2. 关注重复性任务：找出经常需要进行信息提炼和总结的工作流程。

3. 评估时间价值：考虑哪些领域中，快速获取信息概览可以带来显著的时间和效率提升。

4. 跨部门沟通：寻找需要将专业信息转化为通俗易懂形式的场景，以促进跨部门理解。

5. 规律性分析：考虑哪些业务流程中需要定期对大量信息进行分析和提炼。


根据上述建议，我再来帮你发散一下，起到抛砖引玉的作用。

比如生成会议纪要、报告摘要等，就是从信息过载的场景下解放人力；把大量的用户反馈总结提炼，识别其中的主要问题，就是从重复性任务中总结共性，找到反馈集中的问题；针对行业动态新闻，让大模型阅读，呈现关键信息，然后再人工介入，节约了个人的时间；把企业内部不同部门的文档做成知识库，让大模型用对应部门能理解的语言用词解释，提高了协作效率；项目跟进过程中的各种记录，让大模型进行分析，可以快速发现问题，指导下一步的动作……

## 小结

这节课我给你讲了极客时间用文章生成思维导图的实现方法，然后又探讨了一些优化细节的方法，以及长文信息压缩的价值场景。希望在你学习完本节课之后，能根据自己的业务场景，找到可以应用 LLM 压缩提炼长文的地方，落地并实践你的思考，也欢迎你在评论区分享你的经验！