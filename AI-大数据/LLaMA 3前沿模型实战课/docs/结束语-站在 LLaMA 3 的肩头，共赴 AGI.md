你好，我是Tyler！

很高兴又到了这一季专栏的结束语，可以稍微放松一下，不聊技术细节，和大家说说心里话。这一季，我们跟随几件“大事”的脚步，一起探索了大模型和 AI 技术的新发展。

### 第一件大事：“草莓模型”与后推理技术

随着 GPT-4o1 的问世，“ **后推理技术**”这一概念逐渐进入大众视野。这一技术让大模型不仅仅是一个聊天机器人，还能够胜任复杂的推理任务，成为真正的解题高手。在这一季的前两章，我们从最基础的推理能力入手，逐步引导大家深入理解高级推理策略。

这一部分内容的核心目标，是帮助你将大模型从一个“工具”转化为真正的“助手”。我们希望你不仅能依赖它完成任务，还能通过学习理解其背后的逻辑，让大模型变得更加“听话”、更值得信赖。

### 第二件大事：AI搜索产品的全面开花

AI搜索技术的全面普及，可能是许多人都能直接感受到的变化。无论是 AI 搜索引擎的成熟，还是 ChatGPT 的联网能力升级，都让这项技术的潜力逐步显现。本季专栏，我们特别关注了 RAG（检索增强生成）技术，深入探讨了如何让大模型贯穿数据检索的每一个环节。

从数据的获取与加工，到答案的生成，我们试图带领大家搭建的不仅是一个简单的向量数据库，而是一个具备理解能力和场景适应能力的 “智能搜索引擎”。这部分内容的深度，希望能帮助你更好地打造属于自己的AI搜索产品。

### 第三件大事：多智能体技术的快速发展

多智能体技术的发展是这一年很重要的趋势。为什么不能用一个“超级智能体”搞定所有问题？为什么多智能体反而是更合理的选择？为什么分布式、多智能体的协作模式成为了更优解？在本季最后几章，我们详细剖析了这一现象背后的逻辑。搞清楚这个深层原因，能让你在使用智能体技术时，更有方向感，也更有底气。

### 第四件大事：多模态技术&具身智能技术

在这一季的最后一章，我们聊到了当前大语言模型发展的瓶颈问题。这些年，大语言模型的进步确实让人眼前一亮，靠的就是不断加大数据量、增加模型参数。但是，现在这种“堆量”思路已经碰到了不小的麻烦。

训练大语言模型需要海量的高质量文本，比如学术论文、书籍、专业新闻等等。问题是，这些优质数据基本已经被挖得差不多了，找到更多有价值的数据变得越来越难。更重要的是，即使你能找到更多数据，模型的提升幅度也在逐渐变小，换句话说，回报率在变低。靠简单扩展数据规模来提升模型性能的路，已经不太行得通了。

还有一个问题是，扩展训练数据的Token数量带来的收益在减弱。哪怕你给模型喂更多的文字，它的能力提升也没以前那么明显了。这对那些期待大语言模型自动地进化成具身智能或者通用人工智能（AGI）的人来说，无疑是个打击。

既然语言数据快到头了，研究的目光自然转向其他模态，比如视觉、音频这些领域。尤其是视觉模态，信息丰富，还能弥补文字的不足，也更符合人类的学习习惯。

在这方面，Meta的动作很快。在第十八课中我们讨论了他们推出的 LLaMA 3.2 Vision，把视觉和语言结合到了一起，通过引入视觉模态，不仅扩展了模型的能力，也让它在理解能力和应用场景上有了很大的突破。

除此之外，在本季的 [第十九课](https://time.geekbang.org/column/article/827641) 中，我们还讨论了基于LLaMA模型开发的 OpenVLA。这款模型通过整合视觉信息，借助大语言模型的知识，生成智能体的控制指令，为智能系统的未来发展铺平了道路。

### 聊聊开源

这一季中，我们特别关注了 LLaMA 模型的应用与设计。LLaMA 就像一把锋利的宝剑，但只有掌握深厚的“内功”，才能真正用好这把武器。因此，我们从模型设计背景到实际应用场景，带你全方位探索其潜力。

我不止希望你知道 LLaMA “能做什么”，更希望你理解它 “为什么能”，以及 “如何用得更好”。这样，你在面对复杂任务时，不仅能熟练使用它，还能基于理解优化自己的策略。

谈到以上诸多技术的飞速发展，绕不开开源技术的支持。在这里，我也想聊一聊对开源技术的看法。事实上，本季内容从某种意义上，正是在围绕全球规模最大、影响最深的大模型开源技术之一——LLaMA 系列模型展开。

在 [第一季](https://time.geekbang.org/column/intro/100613101?tab=catalog) 的课程中，我对开源技术学习的态度是非常明确的：不要在没有得到商业公司持续支持的开源项目上投入过多精力，尤其是在学习阶段。原因很简单，这种投入风险较高，收益难以保障，有点类似于“买彩票”。

开源技术看似自由开放，但它的背后通常存在着非常强大的驱动因素。尤其是商业公司主导的开源项目，往往有着明确的商业动机，比如吸引顶尖人才、巩固市场地位、掌握行业话语权等。正是因为这种驱动力的存在，这类开源项目在技术的先进性、工具的实际价值以及长期维护上都有更高的保障。对于个人学习来说，这样的项目能够为职业发展提供更可靠的支持。而对于一些没有强大支持的开源项目来说，即使技术本身可能很有意思，但它的生命周期和维护力度往往难以预测。如果你在这样的项目上投入过多时间，可能会浪费掉宝贵的学习精力。

拿 LLaMA 系列模型来说，它从一开始就得到了 Meta 的深度支持。从 LLaMA 1 到当前版本的持续优化与稳定迭代，这个项目已发展为一个高度成熟的开源大模型生态。这不仅体现在技术迭代上，也体现在社区的广泛认可和实际应用场景的不断扩展上。LLaMA 模型可以说是一个适合从初学者到进阶开发者甚至资深研究者都值得长期跟进和学习的项目。通过它，你可以深入了解大模型技术的架构设计、性能优化以及开源生态中的实际开发。

因此，我想强调，选择一个合适的开源技术进行深度学习时，不仅要关注它是否“有趣”，更要关注它是否“有用”。有用，指的是它是否有行业内的实际价值，是否能够长期维护，是否能对你的职业发展带来实质帮助。LLaMA 系列模型无疑是目前大模型开源领域中的既“有趣”又“有用”的一个优质选择，这也是我们在这一季的内容中，花了大量的篇幅探讨它的原因。

这一季，我们特别注重开源技术的实践，因为开源社区的发展正让大模型技术的门槛不断降低，吸引着全球的开发者参与进来，共同推动技术进步。为了帮助你在课程结束后依然能够继续提升，我在第 17 课中预告了一个名为“ [LLaMA 系列模型开发技巧练级攻略](https://github.com/tylerelyt/LLaMa-in-Action)”的项目。这个项目会托管在我的 GitHub 上，即使课程告一段落，你也可以通过项目中的示例代码继续学习，甚至直接参与到开源社区的贡献中。我希望通过这些资源，帮助你逐步突破技术边界，把大模型技术真正融入到自己的技能体系中，从而在实际应用中不断精进。

### 最后的话

第一季的时候，我借用了类似《双城记》开篇的方式，和你分享了我对当时技术趋势的判断。现在看来，当时的表达依然适用。

> 这是通用人工智能（AGI）曙光初现的时代，也可能是人类即将被智能体奴役的时代；这是 AIGC 创意迸发的时代，也可能是虚假内容满天飞的时代；这是技术人才机遇最多的时代，也可能是技术人才最容易被淘汰的时代。

未来的技术之路还充满未知，但同时也有无限可能。AGI 的曙光已经显现，技术发展的速度越来越快，希望你能跟上节奏。这一季的内容可能没办法解答你所有的疑问，但我真心希望它能成为你探索大模型技术的一枚“助推火箭”，让你在通向 AGI 的路上越走越远，越走越快，一起迎接更加智能的未来！

最后我也想听听你的声音，希望你可以花两分钟时间填写一下结课问卷，希望看到你的反馈。

[![图片](https://static001.geekbang.org/resource/image/09/05/09171495356193d034f1f35e65405505.jpg?wh=1142x801)](https://jsj.top/f/h0bWVk)